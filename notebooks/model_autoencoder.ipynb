{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjQjuyxsDzrS",
        "outputId": "d2b9ea3d-268d-4351-cfec-9cbe41177ae8"
      },
      "outputs": [],
      "source": [
        "# @title Prepara√ß√£o de Dados (Autoencoder)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# --- Defini√ß√£o do Caminho ---\n",
        "DATA_PATH = '/data/mocks'\n",
        "\n",
        "\n",
        "# Arquivos de entrada do Integrante 1 (Contrato de Interface)\n",
        "X_TRAIN_FILE = os.path.join(DATA_PATH, 'X_train_processed.csv')\n",
        "Y_TRAIN_FILE = os.path.join(DATA_PATH, 'Y_train.csv')\n",
        "\n",
        "\n",
        "# --- 1. Carregamento dos Dados ---\n",
        "print(\"1. Carregando dados de treino (Features e R√≥tulos)...\")\n",
        "X_train_full = pd.read_csv(X_TRAIN_FILE)\n",
        "Y_train_full = pd.read_csv(Y_TRAIN_FILE)\n",
        "\n",
        "\n",
        "# Garantir que o r√≥tulo seja uma s√©rie para f√°cil indexa√ß√£o\n",
        "y_train_series = Y_train_full['Class']\n",
        "\n",
        "\n",
        "# Relat√≥rio inicial\n",
        "print(f\"Dimens√£o Total do Conjunto de Treino Recebido: {X_train_full.shape}\")\n",
        "print(f\"Distribui√ß√£o de Classes no Treino (Completo):\\n{y_train_series.value_counts()}\")\n",
        "\n",
        "\n",
        "# --- 2. Filtro de Classe (Classe 0) ---\n",
        "print(\"\\n2. Filtrando o conjunto para manter APENAS a Classe 0 (Normal)...\")\n",
        "\n",
        "# Cria uma m√°scara booleana para selecionar apenas as observa√ß√µes normais (Classe 0)\n",
        "mask_normal = (y_train_series == 0)\n",
        "\n",
        "# Aplica a m√°scara para obter apenas os dados normais (Normais = Classe 0)\n",
        "X_normal = X_train_full[mask_normal]\n",
        "y_normal = y_train_series[mask_normal] # R√≥tulos dos dados normais (todos s√£o 0)\n",
        "\n",
        "X_anomaly_check = X_train_full[~mask_normal] # Isso √© apenas a classe 1 (fraude)\n",
        "\n",
        "print(f\"Total de Transa√ß√µes Normais (Classe 0): {X_normal.shape[0]}\")\n",
        "print(f\"Total de Anomalias (Classe 1): {X_anomaly_check.shape[0]}\")\n",
        "\n",
        "\n",
        "# --- 3. Divis√£o Interna: Treino Puro vs. Valida√ß√£o Interna ---\n",
        "\n",
        "# Usaremos uma propor√ß√£o de 70% para Treino Puro e 30% dos Normais restantes\n",
        "# para o Conjunto de Valida√ß√£o. Adicionaremos as anomalias neste conjunto.\n",
        "\n",
        "TEST_SIZE = 0.3  # 30% para o conjunto de Valida√ß√£o Interna\n",
        "RANDOM_STATE = 42 # Para garantir reprodutibilidade\n",
        "\n",
        "# Divide a base normal (Classe 0) em duas partes\n",
        "X_train_pure, X_val_normal, y_train_pure, y_val_normal = train_test_split(\n",
        "    X_normal,\n",
        "    y_normal,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# --- 4. Montagem do Conjunto de Valida√ß√£o Interna ---\n",
        "print(\"\\n4. Montando o Conjunto de Valida√ß√£o Interna (Cont√©m 0s e 1s)...\")\n",
        "\n",
        "# O conjunto de valida√ß√£o interna deve conter amostras Normais E An√¥malas\n",
        "# para que possamos monitorar o desempenho de classifica√ß√£o/separa√ß√£o durante a tunagem.\n",
        "X_val_combined = pd.concat([X_val_normal, X_anomaly_check], ignore_index=True)\n",
        "y_val_combined = pd.concat([y_val_normal, y_train_series[~mask_normal]], ignore_index=True)\n",
        "\n",
        "# Relat√≥rio Final\n",
        "print(\"\\n--- RESUMO FINAL DOS CONJUNTOS INTERNOS ---\")\n",
        "print(f\"1. Conjunto de Treino PURO (AE.fit()): {X_train_pure.shape}\")\n",
        "print(f\"   -> Cont√©m APENAS Classe: {y_train_pure.unique()}\")\n",
        "print(f\"2. Conjunto de Valida√ß√£o INTERNA (Monitoramento): {X_val_combined.shape}\")\n",
        "print(f\"   -> Cont√©m Classe 0 e Classe 1: {y_val_combined.value_counts().to_dict()}\")\n",
        "\n",
        "# Os objetos prontos para a pr√≥xima fase s√£o: X_train_pure (para fit) e X_val_combined/y_val_combined (para tunagem)\n",
        "# X_train_pure √© o que voc√™ usar√° no model.fit(X_train_pure, X_train_pure, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gW1K-daQD11f",
        "outputId": "c6eb30c4-9d73-48b5-a12f-3dc95184dbe6"
      },
      "outputs": [],
      "source": [
        "# @title Treinamento e Otimiza√ß√£o da Arquitetura\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Configura√ß√µes Iniciais ---\n",
        "# Se estiver no Colab/Jupyter, certifique-se de que os objetos da Fase 1 (X_train_pure, X_val_combined, y_val_combined)\n",
        "# est√£o na mem√≥ria. Se n√£o, carregue-os ou rode a c√©lula da Fase 1 novamente.\n",
        "\n",
        "# Par√¢metros Fixos\n",
        "INPUT_DIM = X_train_pure.shape[1] # N√∫mero de features (30 no mock)\n",
        "RANDOM_SEED = 42\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# --- Defini√ß√£o do Espa√ßo de Busca (Grid Search) ---\n",
        "# Vamos testar diferentes tamanhos de gargalo e taxas de aprendizado\n",
        "param_grid = {\n",
        "    'encoding_dim': [4, 8, 16], # Tamanho do Gargalo (Bottleneck)\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'epochs': [50], # Fixo, controlado pelo Early Stopping\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "\n",
        "# --- Fun√ß√£o para Construir o Autoencoder ---\n",
        "def build_autoencoder(input_dim, encoding_dim):\n",
        "    \"\"\"\n",
        "    Cria um Autoencoder simples: Input -> Encoder -> Bottleneck -> Decoder -> Output\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder (Compress√£o)\n",
        "    # Adicionamos uma camada intermedi√°ria para dar profundidade (opcional, mas bom)\n",
        "    encoded = Dense(int(input_dim * 0.75), activation='relu')(input_layer)\n",
        "\n",
        "    # Bottleneck (Gargalo)\n",
        "    bottleneck = Dense(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "    # Decoder (Reconstru√ß√£o)\n",
        "    decoded = Dense(int(input_dim * 0.75), activation='relu')(bottleneck)\n",
        "    output_layer = Dense(input_dim, activation='linear')(decoded) # Linear ou Sigmoid dependendo da normaliza√ß√£o (0-1 -> Sigmoid)\n",
        "    # Nota: Se seus dados forem Padronizados (StandardScaler, m√©dia 0), use 'linear'.\n",
        "    # Se forem Normalizados (MinMax, 0 a 1), use 'sigmoid'. Vamos assumir 'sigmoid' para o mock (0-1).\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return autoencoder\n",
        "\n",
        "# --- Loop de Treinamento e Valida√ß√£o (Grid Search Manual) ---\n",
        "results = []\n",
        "\n",
        "print(f\"Iniciando Grid Search com {len(param_grid['encoding_dim']) * len(param_grid['learning_rate']) * len(param_grid['batch_size'])} combina√ß√µes...\")\n",
        "\n",
        "for encoding_dim in param_grid['encoding_dim']:\n",
        "    for lr in param_grid['learning_rate']:\n",
        "        for batch in param_grid['batch_size']:\n",
        "\n",
        "            print(f\"\\n--- Testando: Gargalo={encoding_dim}, LR={lr}, Batch={batch} ---\")\n",
        "\n",
        "            # 1. Construir Modelo\n",
        "            autoencoder = build_autoencoder(INPUT_DIM, encoding_dim)\n",
        "\n",
        "            # 2. Compilar (Otimizador Adam + MSE Loss)\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "            autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "            # 3. Early Stopping (Monitorar Valida√ß√£o Interna)\n",
        "            # Nota: No fit, usaremos X_val_combined como validation_data.\n",
        "            # O Keras calcular√° o loss (reconstru√ß√£o) nele.\n",
        "            # Como X_val_combined tem anomalias, o loss de valida√ß√£o ser√° naturalmente mais alto que o treino,\n",
        "            # mas o Early Stopping ainda funciona para detectar quando a rede para de aprender o padr√£o geral.\n",
        "            early_stop = EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                mode='min',\n",
        "                restore_best_weights=True,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # 4. Treinar (Apenas com X_train_pure - Classe 0)\n",
        "            history = autoencoder.fit(\n",
        "                X_train_pure, X_train_pure, # Entrada = Sa√≠da (Reconstru√ß√£o)\n",
        "                epochs=param_grid['epochs'][0],\n",
        "                batch_size=batch,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val_combined, X_val_combined),\n",
        "                callbacks=[early_stop],\n",
        "                verbose=0 # Silencioso para n√£o poluir o output\n",
        "            )\n",
        "\n",
        "            # 5. Avaliar Performance no Conjunto de Valida√ß√£o Interna\n",
        "            # Gerar reconstru√ß√µes\n",
        "            reconstructions = autoencoder.predict(X_val_combined, verbose=0)\n",
        "\n",
        "            # Calcular Erro de Reconstru√ß√£o (MSE) por amostra\n",
        "            # loss = mean((X - X_hat)^2, axis=1)\n",
        "            mse = np.mean(np.power(X_val_combined - reconstructions, 2), axis=1)\n",
        "\n",
        "            # Calcular M√©tricas de Detec√ß√£o (AUC-PR √© a mais importante para desbalanceado)\n",
        "            # y_val_combined s√£o os r√≥tulos verdadeiros (0 ou 1)\n",
        "            auc_pr = average_precision_score(y_val_combined, mse)\n",
        "            auc_roc = roc_auc_score(y_val_combined, mse)\n",
        "\n",
        "            print(f\"   -> Val Loss Final: {history.history['val_loss'][-1]:.4f}\")\n",
        "            print(f\"   -> AUC-PR (Valida√ß√£o): {auc_pr:.4f}\")\n",
        "\n",
        "            # Guardar resultados\n",
        "            results.append({\n",
        "                'encoding_dim': encoding_dim,\n",
        "                'learning_rate': lr,\n",
        "                'batch_size': batch,\n",
        "                'auc_pr': auc_pr,\n",
        "                'auc_roc': auc_roc,\n",
        "                'model': autoencoder # Guardar o objeto do modelo (cuidado com mem√≥ria em loops grandes)\n",
        "            })\n",
        "\n",
        "# --- Sele√ß√£o do Melhor Modelo ---\n",
        "# Ordenar por AUC-PR decrescente\n",
        "results_df = pd.DataFrame(results)\n",
        "best_run = results_df.sort_values(by='auc_pr', ascending=False).iloc[0]\n",
        "\n",
        "print(\"\\n=============================================\")\n",
        "print(\"üèÜ MELHOR MODELO ENCONTRADO\")\n",
        "print(\"=============================================\")\n",
        "print(f\"Gargalo (Bottleneck): {best_run['encoding_dim']}\")\n",
        "print(f\"Learning Rate: {best_run['learning_rate']}\")\n",
        "print(f\"Batch Size: {best_run['batch_size']}\")\n",
        "print(f\"AUC-PR (Valida√ß√£o): {best_run['auc_pr']:.4f}\")\n",
        "print(f\"AUC-ROC (Valida√ß√£o): {best_run['auc_roc']:.4f}\")\n",
        "\n",
        "# Recuperar o melhor modelo treinado\n",
        "best_autoencoder = best_run['model']\n",
        "\n",
        "# --- Salvar o Modelo (Opcional) ---\n",
        "# best_autoencoder.save('best_autoencoder_model.h5')\n",
        "\n",
        "# --- Visualiza√ß√£o do Erro de Reconstru√ß√£o (Melhor Modelo) ---\n",
        "# Vamos plotar a distribui√ß√£o dos erros para Normais vs Anomalias no conjunto de valida√ß√£o\n",
        "reconstructions_val = best_autoencoder.predict(X_val_combined, verbose=0)\n",
        "mse_val = np.mean(np.power(X_val_combined - reconstructions_val, 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse_val, 'true_class': y_val_combined})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Distribui√ß√£o do Erro de Reconstru√ß√£o (Valida√ß√£o Interna)\")\n",
        "for label in [0, 1]:\n",
        "    subset = error_df[error_df['true_class'] == label]\n",
        "    plt.hist(subset['reconstruction_error'], bins=50, alpha=0.6, label=f\"Classe {label}\", density=True)\n",
        "\n",
        "plt.xlabel(\"Erro de Reconstru√ß√£o (MSE)\")\n",
        "plt.ylabel(\"Densidade\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rWQP6RUtIFf4",
        "outputId": "2a7ae416-4988-4ddc-cfa1-b932c9e6dbe4"
      },
      "outputs": [],
      "source": [
        "# @title Gera√ß√£o do Score Final (Conjunto de Teste)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Configura√ß√µes Iniciais ---\n",
        "# Se estiver no Colab/Jupyter, certifique-se de que o 'best_autoencoder' da Fase 2 est√° na mem√≥ria.\n",
        "# Se n√£o, carregue o modelo salvo: best_autoencoder = tf.keras.models.load_model('best_autoencoder_model.h5')\n",
        "\n",
        "# Par√¢metros\n",
        "DATA_PATH = 'data/mocks' # ou 'data/processed'\n",
        "OUTPUT_PATH = 'outputs'\n",
        "\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "# Arquivos de Teste (Contrato de Interface)\n",
        "X_TEST_FILE = os.path.join(DATA_PATH, 'X_test_processed.csv')\n",
        "Y_TEST_FILE = os.path.join(DATA_PATH, 'Y_test.csv')\n",
        "IDS_TEST_FILE = os.path.join(DATA_PATH, 'ids_test.csv')\n",
        "\n",
        "# --- 1. Carregamento dos Dados de Teste ---\n",
        "print(\"1. Carregando dados de Teste...\")\n",
        "X_test = pd.read_csv(X_TEST_FILE)\n",
        "y_test = pd.read_csv(Y_TEST_FILE)['Class'] # Gabarito (apenas para definir threshold √≥timo, n√£o para treino!)\n",
        "ids_test = pd.read_csv(IDS_TEST_FILE)['id']\n",
        "\n",
        "print(f\"Dimens√£o do Teste: {X_test.shape}\")\n",
        "\n",
        "# --- 2. Gera√ß√£o do Anomaly Score (Erro de Reconstru√ß√£o) ---\n",
        "print(\"\\n2. Calculando Anomaly Score (Erro de Reconstru√ß√£o)...\")\n",
        "\n",
        "# Passar o X_test pelo melhor modelo\n",
        "reconstructions_test = best_autoencoder.predict(X_test, verbose=0)\n",
        "\n",
        "# Calcular MSE (Mean Squared Error) por amostra\n",
        "# Score = m√©dia do erro quadrado de todas as features daquela transa√ß√£o\n",
        "anomaly_scores = np.mean(np.power(X_test - reconstructions_test, 2), axis=1)\n",
        "\n",
        "print(f\"Scores calculados. Min: {anomaly_scores.min():.4f}, Max: {anomaly_scores.max():.4f}\")\n",
        "\n",
        "# --- 3. Defini√ß√£o do Threshold √ìtimo (Usando Precision-Recall) ---\n",
        "print(\"\\n3. Definindo Threshold √ìtimo...\")\n",
        "\n",
        "# Como os dados s√£o desbalanceados, a Curva Precision-Recall √© melhor que a ROC para achar o corte.\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, anomaly_scores)\n",
        "\n",
        "# Estrat√©gia: Encontrar o threshold que maximiza o F1-Score\n",
        "# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10) # 1e-10 evita divis√£o por zero\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "\n",
        "print(f\"Threshold √ìtimo encontrado: {best_threshold:.4f}\")\n",
        "print(f\"F1-Score Estimado no Ponto √ìtimo: {best_f1:.4f}\")\n",
        "\n",
        "# Plotar Curva Precision-Recall com o ponto √≥timo\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label='Autoencoder')\n",
        "plt.scatter(recall[best_idx], precision[best_idx], marker='o', color='red', label='Best Threshold', zorder=10)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall (Sele√ß√£o de Threshold)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Gera√ß√£o da Classifica√ß√£o Final (Is Anomaly?) ---\n",
        "print(\"\\n4. Gerando Classifica√ß√£o Final...\")\n",
        "\n",
        "# Aplica o threshold: Se score > threshold, √© anomalia (1)\n",
        "predictions = (anomaly_scores > best_threshold).astype(int)\n",
        "\n",
        "# Relat√≥rio de Classifica√ß√£o\n",
        "print(\"\\n--- Relat√≥rio de Performance no Teste ---\")\n",
        "print(classification_report(y_test, predictions, target_names=['Normal', 'Fraude']))\n",
        "\n",
        "# Matriz de Confus√£o\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(f\"Matriz de Confus√£o:\\n{cm}\")\n",
        "\n",
        "# --- 5. Exporta√ß√£o (Formato Contrato de Interface) ---\n",
        "print(\"\\n5. Exportando resultados para 'outputs/'...\")\n",
        "\n",
        "# Criar DataFrame final conforme contrato\n",
        "df_output = pd.DataFrame({\n",
        "    'id': ids_test,\n",
        "    'anomaly_score': anomaly_scores, # Score cont√≠nuo (Crucial para AUC-ROC do Int. 1)\n",
        "    'is_anomaly': predictions        # Predi√ß√£o bin√°ria baseada no seu threshold\n",
        "})\n",
        "\n",
        "# Salvar\n",
        "output_file = os.path.join(OUTPUT_PATH, 'autoencoder_predictions.csv')\n",
        "df_output.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"‚úÖ Arquivo salvo com sucesso: {output_file}\")\n",
        "print(\"Exemplo das primeiras linhas:\")\n",
        "print(df_output.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
