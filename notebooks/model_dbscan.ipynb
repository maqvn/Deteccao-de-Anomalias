{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c229b9b3",
   "metadata": {},
   "source": [
    "# 1. Introdução e Objetivo\n",
    "\n",
    "Neste notebook, exploramos o algoritmo **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) para a detecção de anomalias no dataset de cartões de crédito.\n",
    "\n",
    "Como o DBSCAN é um algoritmo de aprendizado não supervisionado baseado em densidade, ele é ideal para identificar \"ruídos\" (fraudes) que não pertencem a nenhum cluster denso de transações normais.\n",
    "\n",
    "**Objetivos:**\n",
    "1. Validar o uso de **PCA** para redução de dimensionalidade e otimização de performance.\n",
    "2. Determinar os hiperparâmetros ideais (`eps` e `min_samples`) utilizando o método do cotovelo (K-Distance Graph).\n",
    "3. Validar o modelo em uma amostra de teste sincronizada com a equipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c75b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializa para evitar erro lá na frente\n",
    "df = None\n",
    "\n",
    "# Tenta ler o dado processado (Ideal para o GitHub)\n",
    "path_processed = '../data/processed/X_test_processed.csv'\n",
    "path_raw_local = '../data/raw/creditcard.csv'\n",
    "path_colab = '/content/drive/MyDrive/AMCD/creditcard.csv'\n",
    "\n",
    "if os.path.exists(path_processed):\n",
    "    print(\"Modo Integração: Lendo dados processados...\")\n",
    "    X = pd.read_csv(path_processed)\n",
    "    # Se ler o processado, NÃO precisa normalizar de novo, apenas PCA\n",
    "    NEED_SCALING = False \n",
    "elif os.path.exists(path_raw_local):\n",
    "    print(\"Modo Dev Local: Lendo dados brutos...\")\n",
    "    df = pd.read_csv(path_raw_local)\n",
    "    X = df.drop(columns=['Class', 'Time'])\n",
    "    NEED_SCALING = True\n",
    "else:\n",
    "    print(\"Modo Colab: Lendo do Drive...\")\n",
    "    df = pd.read_csv(path_colab)\n",
    "    X = df.drop(columns=['Class', 'Time'])\n",
    "    NEED_SCALING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b9c5f",
   "metadata": {},
   "source": [
    "# 2. Análise de Distância (O Gráfico do Cotovelo)\n",
    "\n",
    "Para definir o raio de vizinhança (`eps`), utilizamos o método do **Gráfico de Distância-K**. Calculamos a distância de cada ponto para o seu $k$-ésimo vizinho mais próximo.\n",
    "\n",
    "* **Min_samples (k):** Definido como 14.\n",
    "* **PCA:** Aplicamos PCA reduzindo para 10 componentes para viabilizar o cálculo de distâncias euclidianas.\n",
    "\n",
    "O ponto de \"cotovelo\" (curvatura máxima) no gráfico indica o valor ideal de Epsilon, onde os pontos deixam de ser \"vizinhos\" e começam a ser considerados outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8caabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só aplica scaler se leu o dado bruto (se leu processado, já vem normalizado)\n",
    "if NEED_SCALING:\n",
    "    scaler = StandardScaler()\n",
    "    X_input = scaler.fit_transform(X)\n",
    "else:\n",
    "    X_input = X\n",
    "\n",
    "# PCA é sempre sua responsabilidade\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CÁLCULO DO K-DISTANCE GRAPH (Método do Cotovelo) ---\n",
    "\n",
    "# Configuração\n",
    "k = 14  # Min_samples definido\n",
    "\n",
    "# Amostragem segura:\n",
    "# Se o dataset for muito grande (>50k linhas), pegamos uma amostra de 20% para o gráfico não travar.\n",
    "# Se for o dataset de teste processado (aprox 42k), usamos tudo.\n",
    "if len(X_pca) > 50000:\n",
    "    print(\"Dataset grande detectado. Usando amostra de 20% para visualização...\")\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(X_pca), int(len(X_pca)*0.2), replace=False)\n",
    "    X_plot = X_pca[sample_indices]\n",
    "else:\n",
    "    print(\"Dataset de tamanho comportado. Usando todos os pontos para visualização.\")\n",
    "    X_plot = X_pca\n",
    "\n",
    "# Cálculo dos Vizinhos\n",
    "print(f\"Calculando distâncias para k={k}...\")\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(X_plot)\n",
    "distances, indices = nbrs.kneighbors(X_plot)\n",
    "\n",
    "# Ordenação para o gráfico\n",
    "distance_desc = sorted(distances[:, k-1])\n",
    "\n",
    "# Função auxiliar para encontrar o ponto de curvatura máxima (Knee/Elbow Point)\n",
    "def get_knee_point(values):\n",
    "    n_points = len(values)\n",
    "    all_coords = np.vstack((range(n_points), values)).T\n",
    "    first_point = all_coords[0]\n",
    "    line_vec = all_coords[-1] - all_coords[0]\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_product = np.sum(vec_from_first * np.tile(line_vec_norm, (n_points, 1)), axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    idx_of_best_point = np.argmax(dist_to_line)\n",
    "    return values[idx_of_best_point]\n",
    "\n",
    "# Encontrar valor sugerido\n",
    "eps_sugerido = get_knee_point(distance_desc)\n",
    "print(f\"--- RESULTADO ---\")\n",
    "print(f\"Epsilon sugerido pelo método matemático: {eps_sugerido:.4f}\")\n",
    "\n",
    "# Plotagem\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distance_desc)\n",
    "plt.axhline(y=eps_sugerido, color='r', linestyle='--', label=f'Eps Sugerido ({eps_sugerido:.2f})')\n",
    "plt.title('Gráfico de Distância-K (Método do Cotovelo)')\n",
    "plt.ylabel('Distância Epsilon')\n",
    "plt.xlabel('Pontos (ordenados por distância)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ad54f",
   "metadata": {},
   "source": [
    "# 3. Validação do Modelo\n",
    "\n",
    "Agora aplicamos o DBSCAN com o valor de Epsilon escolhido (`eps=2.96`) para verificar a performance.\n",
    "\n",
    "* **Se estivermos no Colab (Modo Raw):** Faremos a divisão \"Treino/Teste\" simulada para calcular Recall e Precisão, pois temos as labels originais.\n",
    "* **Se estivermos no GitHub (Modo Integração):** O código apenas rodará a predição no conjunto `X_test_processed` (sem labels), simulando o ambiente de produção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629784a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUÇÃO E VALIDAÇÃO ---\n",
    "\n",
    "# Parâmetros Finais\n",
    "EPS_FINAL = 2.9619\n",
    "MIN_SAMPLES = 14\n",
    "\n",
    "print(f\"Rodando DBSCAN (eps={EPS_FINAL}, min_samples={MIN_SAMPLES})...\")\n",
    "print(\"Isso pode levar alguns segundos...\")\n",
    "\n",
    "# Se estivermos rodando no COLAB (Dataset Bruto com Labels 'Class')\n",
    "# Precisamos criar o conjunto de teste igual ao do grupo para validar\n",
    "if df is not None and 'Class' in df.columns:\n",
    "    print(\"\\n[MODO VALIDAÇÃO] Dataset completo com labels encontrado.\")\n",
    "    print(\"Recriando o split de teste oficial (42.722 amostras) para calcular métricas...\")\n",
    "    \n",
    "    # Recria o split (Logica do Integrante 1)\n",
    "    y_real = df['Class']\n",
    "    # 1. Tira Treino (30%)\n",
    "    _, X_temp_pca, _, _, _, y_temp = train_test_split(\n",
    "        X_pca, df.index, y_real, test_size=0.30, stratify=y_real, random_state=42\n",
    "    )\n",
    "    # 2. Tira Validação (50% do resto) -> Sobra Teste (50%)\n",
    "    _, X_teste_final, _, _, _, y_teste_final = train_test_split(\n",
    "        X_temp_pca, range(len(X_temp_pca)), y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Roda DBSCAN na fatia de teste\n",
    "    db = DBSCAN(eps=EPS_FINAL, min_samples=MIN_SAMPLES, n_jobs=-1)\n",
    "    labels = db.fit_predict(X_teste_final)\n",
    "    \n",
    "    # Métricas\n",
    "    y_pred = [1 if x == -1 else 0 for x in labels]\n",
    "    \n",
    "    print(\"\\n--- RELATÓRIO DE CLASSIFICAÇÃO (Amostra de Teste) ---\")\n",
    "    print(classification_report(y_teste_final, y_pred, target_names=['Normal', 'Fraude']))\n",
    "    \n",
    "    cm = confusion_matrix(y_teste_final, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"Matriz de Confusão:\")\n",
    "    print(f\"Fraudes Detectadas (Recall): {tp}\")\n",
    "    print(f\"Falsos Positivos: {fp}\")\n",
    "    print(f\"Fraudes Perdidas: {fn}\")\n",
    "\n",
    "else:\n",
    "    # MODO INTEGRAÇÃO (Apenas X disponível)\n",
    "    print(\"\\n[MODO PRODUÇÃO] Apenas features disponíveis (sem labels).\")\n",
    "    print(f\"Rodando predição em {len(X_pca)} amostras...\")\n",
    "    \n",
    "    db = DBSCAN(eps=EPS_FINAL, min_samples=MIN_SAMPLES, n_jobs=-1)\n",
    "    labels = db.fit_predict(X_pca)\n",
    "    \n",
    "    n_anomalies = np.sum(labels == -1)\n",
    "    print(f\"Concluído! Anomalias detectadas: {n_anomalies}\")\n",
    "    print(\"Para ver métricas de acerto, rode no Colab com o dataset original.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8a2e3",
   "metadata": {},
   "source": [
    "# 4. Conclusão e Definição de Parâmetros\n",
    "\n",
    "Com base na experimentação acima, definimos a configuração final para o script de produção:\n",
    "\n",
    "1.  **Pré-processamento:** Normalização (`StandardScaler`) seguida de **PCA (10 componentes)**. A redução de dimensionalidade provou-se essencial para viabilizar o cálculo da matriz de distâncias.\n",
    "2.  **Hiperparâmetros:**\n",
    "    * `eps = 2.9619`: Identificado visual e matematicamente pelo método do cotovelo.\n",
    "    * `min_samples = 14`: Valor robusto para evitar que micro-clusters de ruído sejam considerados normais.\n",
    "3.  **Resultados:** O modelo demonstrou capacidade de detectar fraudes (Recall) no conjunto de teste, ainda que com uma taxa esperada de falsos positivos, característica intrínseca de métodos de densidade em dados desbalanceados.\n",
    "\n",
    "**Próximos Passos:**\n",
    "O algoritmo validado aqui foi implementado no script `src/models/dbscan.py`, configurado para processar o conjunto de teste oficial e gerar o arquivo de submissão `outputs/dbscan_predictions.csv`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
