# -*- coding: utf-8 -*-
"""AMCD

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11artJWUHQnNMfijwnPoCynY-8weIVQXd
"""

# importação de bibliotecas e config. de visualização

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("Bibliotecas importadas com sucesso!")

"""## Visão geral do Dataset

O dataset utilizado é o Credit Card Fraud Detection, contendo transações realizadas
por clientes europeus em setembro de 2013.

Cada linha representa uma transação individual, e as colunas são:

- **Time**:  
  Tempo em segundos desde a primeira transação registrada no dataset.  
  Representa uma informação temporal relativa, e não uma data absoluta.

- **V1 a V28**:  
  Variáveis numéricas resultantes de uma transformação por PCA (Principal Component Analysis),
  aplicada com o objetivo de preservar a privacidade dos usuários.  
  Por esse motivo:
  - O significado original dessas variáveis não é conhecido
  - Elas são aproximadamente descorrelacionadas entre si
  - Apresentam média próxima de zero e variância controlada

- **Amount**:  
  Valor monetário da transação.  
  Essa variável não passou pelo PCA e, portanto, pode apresentar:
  - Assimetria
  - Outliers
  - Escala diferente das demais variáveis

- **Class**:  
  Variável alvo (target):
  - 0 = Transação normal
  - 1 = Transação fraudulenta

O dataset é caracterizado por um desbalanceamento extremo, onde fraudes representam
uma fração muito pequena do total de transações.

"""

df = pd.read_csv('data/raw/creditcard.csv')
df.head()

"""## ANÁLISE EXPLORATÓRIA"""

print('\n INFORMAÇÕES GERAIS, TIPOS E ESTATÍSTICAS DESCRITIVAS')
print(f"Shape: {df.shape}")
print(f"Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\n")
df.info()

"""Observa-se que todas as variáveis são numéricas, descartando a necessidade de encoding"""

print("\n")
df.describe()

"""A tabela de estatísticas descritivas apresenta medidas fundamentais para compreender o comportamento das variáveis numéricas do dataset:

- **count**:  
  Indica a quantidade de valores não nulos em cada coluna.  
  Observa-se que todas as variáveis possuem o mesmo valor de count, indicando
  **ausência de valores faltantes** no dataset.

- **mean (média)**:  
  Representa o valor médio das observações.  
  As variáveis V1 a V28 apresentam médias próximas de zero, o que é esperado,
  pois são resultado de uma transformação por PCA.  
  A variável Amount apresenta média superior à mediana, sugerindo uma
  **distribuição assimétrica à direita**.

- **std (desvio padrão)**:  
  Mede a dispersão dos dados em torno da média.  
  As variáveis PCA possuem desvios padrão relativamente homogêneos, enquanto
  Amount apresenta maior variabilidade, reforçando a necessidade de normalização.

- **min e max**:  
  Representam os valores mínimo e máximo observados.  
  A presença de valores máximos elevados em Amount indica possíveis outliers.

- **25%, 50% (mediana) e 75%**:  
  Quartis da distribuição.  
  A diferença entre a média e a mediana em Amount reforça a assimetria da variável,
  enquanto as variáveis PCA apresentam distribuições mais centradas.

"""

# verificar desbalanceamento
print("\n DISTRIBUIÇÃO DE CLASSES")
df['Class'].value_counts(normalize=True) * 100

"""A análise das classes revela um desbalanceamento extremo no df: A grande maioria das transações pertence à classe 0 (normal), e apenas uma fração muito pequena corresponde à classe 1 (fraude)

Em termos percentuais, as transações fraudulentas representam aproximadamente 0,17% do total.
"""

# unique values
print("\n VALORES ÚNICOS POR COLUNA")
for col in df.columns:
    unique_count = df[col].nunique()
    print(f"{col}: {unique_count} valores únicos")

"""As variáveis V1 a V28 apresentam um número extremamente alto de valores únicos, próximo ao número total de observações do dataset. Esse comportamento é esperado, uma vez que essas variáveis são o resultado de uma transformação por PCA, gerando atributos contínuos que raramente se repetem exatamente.

A variável Time também apresenta uma alta cardinalidade, indicando que muitas transações ocorrem em instantes de tempo distintos.

Amount tem um número significativamente menor de valores únicos em comparação às variáveis PCA, o que sugere a existência de valores repetidos e talvez possíveis concentrações em determinados montantes de transação.

Por fim, a variável Class apresenta apenas dois valores únicos, correspondentes às classes 0 (normal) e 1 (fraude).
"""

# Histograma de Amount
plt.figure(figsize=(8, 4))
sns.histplot(df['Amount'], bins=50, kde=True)
plt.title('Distribuição da variável Amount')
plt.xlabel('Amount')
plt.ylabel('Frequência')
plt.show()

"""O histograma da variável Amount evidencia uma distribuição fortemente assimétrica
à direita, caracterizada por uma grande concentração de transações de baixo valor
e uma cauda longa associada a valores elevados. Esse comportamento é típico de dados financeiros e justifica a presença de outliers legítimos, que não devem ser tratados como erros de medição no contexto de detecção de fraudes.

"""

# Histograma de Time
plt.figure(figsize=(8, 4))
sns.histplot(df['Time'], bins=50, kde=True)
plt.title('Distribuição da variável Time')
plt.xlabel('Time')
plt.ylabel('Frequência')
plt.show()

"""Em comparação ao Amount, o histograma da variável Time indica uma distribuição mais contínua ao longo
do intervalo temporal considerado, refletindo o caráter sequencial e relativo dessa variável. Porém, há sim certa variação, incluindo uma queda na densidade de transações em torno
de determinado ponto temporal (há aumento, redução e depois novo aumento na frequência). Esse comportamento reflete períodos com menor volume
de transações, possivelmente associados a janelas temporais distintas de coleta ou
a ciclos naturais de atividade ao longo do dia.

Captura a dinâmica temporal real do sistema, não sendo
resultado de erro ou inconsistência nos dados.


"""

# OUTLIERS (identificar, não remover)
# Boxplot para identificar outliers na variável Amount
plt.figure(figsize=(8, 4))
sns.boxplot(x=df['Amount'])
plt.title('Boxplot da variável Amount')
plt.show()

"""A análise da variável Amount evidencia a presença de outliers, caracterizados por transações de valor significativamente mais elevado que a maioria dos registros.

No contexto de detecção de fraudes, esses outliers não representam erros de dados, mas sim eventos mais raros e informativos: é sim possível ter transações de valor mais elevado. Dessa forma, não removemos esses valores, preservando informações relevantes para os modelos de detecção de anomalias.
"""

# Boxplot de Amount por classe
plt.figure(figsize=(8, 4))
sns.boxplot(x='Class', y='Amount', data=df)
plt.title('Amount por Classe')
plt.xlabel('Class (0 = Normal, 1 = Fraude)')
plt.ylabel('Amount')
plt.show()

"""Observa-se que transações **normais (Classe 0)** apresentam uma ampla faixa de valores,
incluindo os maiores montantes registrados no conjunto de dados, com diversos outliers
de alto valor. Já as transações **fraudulentas (Classe 1)** concentram-se,
majoritariamente, em valores mais baixos, com menor dispersão e poucos registros de
valores elevados.

Esse comportamento sugere que fraudes não estão necessariamente associadas a
transações de alto valor, mas podem ocorrer com maior frequência em valores moderados
ou baixos, possivelmente como estratégia para evitar detecção.
"""

fraud = df[df['Class']==0]
non_fraud = df[df['Class']==1]
fraud.shape, non_fraud.shape
plt.figure(figsize=(10,4))
sns.kdeplot(non_fraud['V1'], label='Non-Fraud', fill=True)
sns.kdeplot(fraud['V1'], label='Fraud', fill=True)
plt.title('densidade V1 por Classe')
plt.figure(figsize=(10,4))
sns.kdeplot(non_fraud['V2'], label='Non-Fraud', fill=True)
sns.kdeplot(fraud['V2'], label='Fraud', fill=True)
plt.title('densidade V2 por Classe')

# Boxplot de uma variável PCA por classe
plt.figure(figsize=(8, 4))
sns.boxplot(x='Class', y='V1', data=df)
plt.title('V1 por Classe')
plt.show()
plt.figure(figsize=(8, 4))
sns.boxplot(x='Class', y='V2', data=df)
plt.title('V2 por Classe')
plt.show()

"""Algumas variáveis PCA apresentam distribuições distintas entre transações normais
e fraudulentas, indicando que padrões latentes capturados pelo PCA podem ser
explorados pelos modelos de detecção de anomalias.
Embora as variáveis V1 a V28 não possuam interpretação semântica direta, a análise
bivariada em relação à variável alvo indica que algumas componentes apresentam
distribuições distintas entre transações normais e fraudulentas. Isso sugere que
padrões latentes capturados pelo PCA podem ser explorados pelos modelos de detecção
de anomalias.
"""

# TIME X CLASS

plt.figure(figsize=(8, 4))
sns.histplot(df[df['Class'] == 0]['Time'], bins=50, label='Normal', stat='density')
sns.histplot(df[df['Class'] == 1]['Time'], bins=50, label='Fraude', stat='density')
plt.legend()
plt.title('Distribuição de Time por Classe')
plt.show()

"""A análise da distribuição temporal em função da classe revela que, em determinados
intervalos com menor volume total de transações, como logo após o início ou aproximadamente em Time 100000, observa-se um aumento relativo na densidade de transações
fraudulentas. Isso não indica necessariamente um aumento absoluto
no número de fraudes, mas sim uma maior proporção de fraudes em períodos de menor
atividade geral. Tal padrão pode estar associado a dinâmicas temporais específicas ou a potenciais fragilidades na
segurança em determinados horários, reforçando a importância da análise temporal
para identificação de janelas de maior risco.

Enquanto transações não fraudulentas apresentam uma distribuição mais consistente e regular, refletindo padrões naturais de comportamento dos usuários ao longo do
tempo, as transações fraudulentas mostram-se mais dispersas e irregulares, o que pode sugerir
que a fraude é intencionalmente
distribuída ao longo do tempo para evitar detecção.
"""

# Matriz de correlação (amostragem para eficiência visual)

corr = df.corr(numeric_only=True)
plt.figure(figsize=(12,10))
sns.heatmap(corr, cmap='coolwarm', annot=False, linewidths=0.5)
plt.title('Matriz de Correlação')
plt.show()

cor_target = corr['Class'].abs().sort_values(ascending=False)
cor_target = cor_target.drop('Class')
top_features= cor_target.head(5).index.tolist()
print("Top 5 variáveis mais correlacionadas com 'Class':")
print(top_features)

"""A matriz de correlação indica baixa correlação entre as variáveis V1 a V28,
resultado esperado da aplicação do PCA. As variáveis Amount e Time apresentam
correlações limitadas com as demais, reforçando sua natureza distinta. Não se observa
a presença de pares de variáveis com correlação linear elevada, o que reduz riscos
de multicolinearidade e favorece a aplicação de modelos probabilísticos e baseados
em distância.

Em relação à variável alvo (Class), nota-se que as correlações lineares com as
demais variáveis são, em geral, de baixa magnitude. Esse resultado indica que a
separação entre transações normais e fraudulentas não depende de relações lineares
simples, reforçando a necessidade de modelos capazes de capturar padrões não lineares
e interações multivariadas complexas.
Algumas variáveis, em especial V17/V14/V12,  apresentam correlações negativas mais acentuadas com a classe alvo, sugerindo que essas componentes principais capturam padrões relevantes associados a comportamentos fraudulentos.
"""

plt.figure(figsize=(10,4))
sns.kdeplot(non_fraud['V17'], label='Non-Fraud', fill=True)
sns.kdeplot(fraud['V17'], label='Fraud', fill=True)
plt.figure(figsize=(8, 4))
sns.boxplot(x='Class', y='V17', data=df)
plt.title('V17 por Classe')
plt.show()

"""Fazendo agora uma análise univariada especificamente na variável V17:

No gráfico de densidade, nota-se que as transações não fraudulentas
apresentam valores de V17 fortemente concentrados próximos de zero, com baixa
variabilidade e caudas relativamente suaves. Em contrapartida, as transações
fraudulentas mostram um deslocamento da distribuição
para a região negativa, evidenciando
diferenças marcantes no comportamento dessa variável entre as classes.

O boxplot reforça essa observação ao evidenciar uma mediana significativamente mais
baixa para a classe fraudulenta, além de maior variabilidade e presença de valores
extremos negativos. Enquanto a maioria das transações normais permanece próxima de
zero, as fraudes apresentam uma concentração expressiva em valores negativos, com
pouca sobreposição entre as distribuições centrais das duas classes.

Esses resultados indicam que valores negativos de V17 são fortemente associados
a transações fraudulentas, sugerindo que essa variável captura padrões latentes
relevantes para a detecção de fraudes.

Esse padrão se repete nas variáveis mais correlacionadas ao target.

## SPLIT DOS DADOS EM TREINO, VALIDAÇÃO E TESTE

Split HOLDOUT 70-15-15 com estratificação (mantém proporção das classes)

Primeiro split: 70% treino, 30% temporário (val+test)

Segundo split: dividir o temporário em validação (15%) e teste (15%).
"""

# criar um identificador da transação usando o índice como ID.
df = df.copy()
df['id'] = df.index

y = df['Class'] #target

# Features: remover Class e id (id não deve entrar no modelo)
X = df.drop(columns=['Class', 'id'])

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,
    random_state=42,
    stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print(f"Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Validação: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)")
print(f"Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)")
print("\nDistribuição do target por conjunto:")
print(f"Treino: {y_train.value_counts(normalize=True).to_dict()}")
print(f"Validação: {y_val.value_counts(normalize=True).to_dict()}")
print(f"Teste: {y_test.value_counts(normalize=True).to_dict()}")

# Separar os IDs do conjunto de teste (para salvar ids_test.csv depois)
ids_test = df.loc[X_test.index, 'id']

# Visualizar distribuições
fig, axes = plt.subplots(1, 4, figsize=(16, 4))
datasets = [('Original', y), ('Treino', y_train), ('Validação', y_val), ('Teste', y_test)]
for i, (name, data) in enumerate(datasets):
    data.value_counts().plot(kind='bar', ax=axes[i])
    axes[i].set_title(f'{name}\n(n={len(data)})')
    axes[i].set_ylabel('Contagem')
    axes[i].tick_params(axis='x', rotation=0)

    # Adicionar percentuais
    total = len(data)
    for j, v in enumerate(data.value_counts()):
        axes[i].text(j, v + 1, f'{v/total*100:.1f}%', ha='center')
plt.suptitle('Distribuição do Target', fontsize=14)
plt.tight_layout()
plt.show()

"""## Limpeza

Como já visto, etapas como encoding e tratamento de outliers não são necessários.
"""

# MISSING VALUES
# Verificação de valores faltantes por coluna
missing_values = df.isnull().sum()
missing_values

"""O dataset não possui valores ausentes em nenhuma das colunas.

"""

# DUPLICATAS
num_duplicates = df.duplicated().sum()
num_duplicates

"""A análise de duplicatas revelou que não há registros duplicados no conjunto de dados.

"""

# VALORES IMPOSSÍVEIS / INCONSISTENTES
# Amount: pode até ser zero, não pode ser negativo
# time: nao pode ser negativo
(df['Amount'] < 0).sum()
(df['Time'] < 0).sum()

"""Foram realizadas verificações de consistência em variáveis com interpretação direta.
Não foram identificados valores negativos na variável Amount nem na variável Time.

As variáveis V1 a V28 são resultado de uma transformação por PCA e, portanto,
não possuem interpretação semântica direta que permita a definição de limites
lógicos específicos.

## Escalonamento
"""

X_train.columns
scaler = StandardScaler()
scaler.fit(X_train)     #apenas no conjunto de treino

# Aplicação da normalização
X_train_scaled = scaler.transform(X_train)
X_val_scaled   = scaler.transform(X_val)
X_test_scaled  = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(
    X_train_scaled,
    columns=X_train.columns,
    index=X_train.index
)

X_val_scaled = pd.DataFrame(
    X_val_scaled,
    columns=X_val.columns,
    index=X_val.index
)

X_test_scaled = pd.DataFrame(
    X_test_scaled,
    columns=X_test.columns,
    index=X_test.index
)

X_train_scaled.describe().loc[['mean', 'std']] #verificação

"""Após a divisão dos dados em conjuntos de treino, validação e teste, foi aplicado o
escalonamento das variáveis por meio do método de padronização (StandardScaler).
O scaler foi ajustado exclusivamente sobre o conjunto de treino, evitando vazamento
de informação, e posteriormente aplicado aos conjuntos de validação e teste.

A padronização é essencial neste projeto, uma vez que os modelos utilizados são
sensíveis à escala das variáveis, garantindo que nenhuma feature domine as demais
por diferenças de magnitude.

## Output
"""

os.makedirs('data/processed', exist_ok=True)

# Features
X_train_scaled.to_csv('data/processed/X_train_processed.csv', index=False)
X_test_scaled.to_csv('data/processed/X_test_processed.csv', index=False)

# Targets
y_train.to_csv('data/processed/y_train.csv', index=False)
y_test.to_csv('data/processed/y_test.csv', index=False)

# IDs do teste
ids_test.to_csv('data/processed/ids_test.csv', index=False)

os.listdir('data/processed')

"""Ao final do pré-processamento, os conjuntos de dados foram exportados para arquivos
CSV, seguindo o contrato de dados definido no projeto. Os arquivos gerados incluem
as features normalizadas para treino e teste, os respectivos rótulos e os
identificadores das amostras de teste, garantindo compatibilidade com as etapas
posteriores de modelagem e avaliação.

## Features utilizadas e decisões da engenharia de features

As features utilizadas no projeto correspondem às variáveis V1 a V28, que são
componentes principais extraídas previamente por meio de PCA (Principal Component
Analysis), além das variáveis originais Time e Amount. As componentes
principais já sintetizam informações relevantes do conjunto de dados ao capturar
padrões latentes e combinações lineares das variáveis originais.

Dessa forma, optou-se por não realizar uma engenharia de features extensiva, uma
vez que transformações adicionais poderiam introduzir redundância, aumentar a
complexidade do espaço de atributos e elevar o risco de overfitting,
especialmente em um cenário de dados extremamente desbalanceados e com foco em
detecção de anomalias, no qual a classe de interesse é rara. A manutenção da
estrutura original das features também contribui para maior robustez e melhor
alinhamento com os pressupostos dos modelos adotados no projeto.

A variável Time foi mantida no conjunto de dados por representar uma informação
temporal relativa que pode capturar padrões indiretos de comportamento ao longo do
período observado, como variações na proporção de fraudes em determinados intervalos
temporais. Embora não seja suficiente isoladamente para discriminar fraudes, essa
variável fornece informação complementar relevante quando analisada em conjunto com
as demais features.

Quanto à variável Amount, apesar da assimetria observada em sua distribuição,
optou-se por não aplicar transformação logarítmica, uma vez que os modelos
utilizados são capazes de lidar com distribuições assimétricas após a etapa de
normalização. Além disso, preservar a escala original de Amount evita a possível
perda de informação associada a transações de valor elevado, que podem ser
potencialmente informativas no contexto de fraude.

Em conjunto, essas decisões visam preservar a estrutura original dos dados,
minimizar introdução de vieses artificiais e garantir maior interpretabilidade e
robustez nas etapas subsequentes de modelagem e detecção de anomalias.
"""