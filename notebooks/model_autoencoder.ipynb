{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjQjuyxsDzrS",
        "outputId": "d2b9ea3d-268d-4351-cfec-9cbe41177ae8"
      },
      "outputs": [],
      "source": [
        "# @title ImportaÃ§Ãµes e ConfiguraÃ§Ãµes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================================================\n",
        "# CONFIGURAÃ‡Ã•ES GERAIS\n",
        "# =========================================================\n",
        "RANDOM_SEED = 42\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "DATA_PATH = 'data/processed'\n",
        "OUTPUT_PATH = 'outputs'\n",
        "\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "    \n",
        "print(\"Ambiente configurado e caminhos definidos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gW1K-daQD11f",
        "outputId": "c6eb30c4-9d73-48b5-a12f-3dc95184dbe6"
      },
      "outputs": [],
      "source": [
        "# @title FunÃ§Ã£o de Carregamento de Dados\n",
        "\n",
        "def load_and_split_data(data_path, test_size=0.2):\n",
        "    # Carregamento seguro\n",
        "    try:\n",
        "        X_train = pd.read_csv(os.path.join(data_path, 'X_train_processed.csv'))\n",
        "        y_train = pd.read_csv(os.path.join(data_path, 'y_train.csv'))['Class']\n",
        "    except FileNotFoundError:\n",
        "        print(\"Arquivos nÃ£o encontrados. Verifique o caminho 'data/processed'.\")\n",
        "        return None\n",
        "\n",
        "    mask_normal = y_train == 0\n",
        "    X_normal = X_train[mask_normal]\n",
        "    y_normal = y_train[mask_normal]\n",
        "\n",
        "    X_anomaly = X_train[~mask_normal]\n",
        "    y_anomaly = y_train[~mask_normal]\n",
        "\n",
        "    # DivisÃ£o\n",
        "    X_train_pure, X_val_normal, _, y_val_normal = train_test_split(\n",
        "        X_normal, y_normal, test_size=test_size, random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    # ValidaÃ§Ã£o combinada (Normais + Fraudes que sobraram do treino original)\n",
        "    X_val_combined = pd.concat([X_val_normal, X_anomaly], ignore_index=True)\n",
        "    y_val_combined = pd.concat([y_val_normal, y_anomaly], ignore_index=True)\n",
        "\n",
        "    print(f\"Treino Puro (Normal): {X_train_pure.shape}\")\n",
        "    print(f\"ValidaÃ§Ã£o (Normal+Fraude): {X_val_combined.shape}\")\n",
        "\n",
        "    return (\n",
        "        X_train_pure.values.astype(np.float32),\n",
        "        X_val_normal.values.astype(np.float32),\n",
        "        X_val_combined.values.astype(np.float32),\n",
        "        y_val_combined.values\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rWQP6RUtIFf4",
        "outputId": "2a7ae416-4988-4ddc-cfa1-b932c9e6dbe4"
      },
      "outputs": [],
      "source": [
        "# @title ExecuÃ§Ã£o do Carregamento\n",
        "\n",
        "# Carrega os dados\n",
        "data = load_and_split_data(DATA_PATH)\n",
        "\n",
        "if data is not None:\n",
        "    X_train_pure, X_val_pure, X_val_combined, y_val_combined = data\n",
        "else:\n",
        "    print(\"Erro crÃ­tico: Dados nÃ£o carregados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Arquitetura do Modelo\n",
        "\n",
        "def build_deep_autoencoder(input_dim, encoding_dim, dropout_rate=0.2):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    \n",
        "    # --- Encoder ---\n",
        "    # Camada de Denoising via Dropout\n",
        "    x = Dropout(dropout_rate)(input_layer)\n",
        "    \n",
        "    # Camadas Profundas\n",
        "    x = Dense(24, activation='relu')(x)\n",
        "    x = BatchNormalization()(x) \n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    # --- Bottleneck (Gargalo) ---\n",
        "    # RegularizaÃ§Ã£o L1 forÃ§a esparsidade\n",
        "    bottleneck = Dense(encoding_dim, activation='relu', activity_regularizer=l1(10e-5))(x)\n",
        "    \n",
        "    # --- Decoder ---\n",
        "    x = Dense(16, activation='relu')(bottleneck)\n",
        "    x = Dense(24, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    # SaÃ­da\n",
        "    output = Dense(input_dim, activation='sigmoid')(x)\n",
        "    \n",
        "    return Model(input_layer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title FunÃ§Ã£o de Treinamento\n",
        "\n",
        "def train_and_evaluate_run(params, X_train_pure, X_val_pure, X_val_combined, y_val_combined):\n",
        "    # Desempacotar parÃ¢metros\n",
        "    encoding_dim = params['encoding_dim']\n",
        "    lr = params['learning_rate']\n",
        "    batch_size = params['batch_size']\n",
        "    epochs = params['epochs']\n",
        "    \n",
        "    autoencoder = build_deep_autoencoder(X_train_pure.shape[1], encoding_dim)\n",
        "\n",
        "    autoencoder.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='mean_squared_error' \n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
        "    ]\n",
        "\n",
        "    # Treino\n",
        "    history = autoencoder.fit(\n",
        "        X_train_pure, X_train_pure,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        validation_data=(X_val_pure, X_val_pure),\n",
        "        callbacks=callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # AvaliaÃ§Ã£o\n",
        "    reconstructions = autoencoder.predict(X_val_combined, verbose=0)\n",
        "    mse = np.mean(np.square(X_val_combined - reconstructions), axis=1)\n",
        "\n",
        "    auc_pr = average_precision_score(y_val_combined, mse)\n",
        "\n",
        "    return auc_pr, autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title DefiniÃ§Ã£o dos HiperparÃ¢metros\n",
        "\n",
        "# =========================================================\n",
        "# MODO DE EXECUÃ‡ÃƒO\n",
        "# 0 = ExecuÃ§Ã£o normal (hiperparÃ¢metros fixos)\n",
        "# 1 = Grid Search (tunagem)\n",
        "# =========================================================\n",
        "RUN_TUNING = 0 \n",
        "\n",
        "if RUN_TUNING == 1:\n",
        "    print(\">>> MODO: GRID SEARCH ATIVADO\")\n",
        "    param_grid = {\n",
        "        'encoding_dim': [4, 8],\n",
        "        'learning_rate': [0.01, 0.001],\n",
        "        'batch_size': [64, 128],\n",
        "        'epochs': [50]\n",
        "    }\n",
        "else:\n",
        "    print(\">>> MODO: EXECUÃ‡ÃƒO ÃšNICA (MELHORES PARÃ‚METROS)\")\n",
        "    param_grid = {\n",
        "        'encoding_dim': [8],\n",
        "        'learning_rate': [0.001],\n",
        "        'batch_size': [128],\n",
        "        'epochs': [50]\n",
        "    }\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "print(f\"Total de combinaÃ§Ãµes a testar: {len(grid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Loop de Treinamento (Grid Search ou Single Run)\n",
        "\n",
        "best_auc_pr = -1\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "print(\"=============================================\")\n",
        "print(f\"INICIANDO EXECUÃ‡ÃƒO\")\n",
        "print(\"=============================================\")\n",
        "\n",
        "for i, params in enumerate(grid):\n",
        "    print(f\"[{i+1}/{len(grid)}] Testando: {params} ...\", end=\" \")\n",
        "    \n",
        "    try:\n",
        "        auc_pr, model = train_and_evaluate_run(\n",
        "            params, X_train_pure, X_val_pure, X_val_combined, y_val_combined\n",
        "        )\n",
        "        print(f\"AUC-PR: {auc_pr:.4f}\")\n",
        "\n",
        "        if auc_pr > best_auc_pr:\n",
        "            best_auc_pr = auc_pr\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "    except Exception as e:\n",
        "        print(f\"Erro: {e}\")\n",
        "\n",
        "print(\"\\nðŸ† MELHOR MODELO ENCONTRADO\")\n",
        "print(f\"Params: {best_params}\")\n",
        "print(f\"AUC-PR (ValidaÃ§Ã£o): {best_auc_pr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title AvaliaÃ§Ã£o Final no Conjunto de Teste\n",
        "\n",
        "def generate_final_scores(best_model, data_path, target_recall=0.80):\n",
        "    # Carrega dados de teste\n",
        "    try:\n",
        "        X_test = pd.read_csv(os.path.join(data_path, 'X_test_processed.csv')).values.astype(np.float32)\n",
        "        y_test = pd.read_csv(os.path.join(data_path, 'y_test.csv'))['Class'].values\n",
        "        ids_test = pd.read_csv(os.path.join(data_path, 'ids_test.csv'))['id']\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar teste: {e}\")\n",
        "        return\n",
        "\n",
        "    # Gera scores (MSE)\n",
        "    reconstructions = best_model.predict(X_test, verbose=0)\n",
        "    anomaly_scores = np.mean(np.square(X_test - reconstructions), axis=1)\n",
        "\n",
        "    # Curva Precision-Recall para definir o Threshold\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, anomaly_scores)\n",
        "\n",
        "    # EstratÃ©gia: Buscar limiar para Recall ~ 80%\n",
        "    valid_idxs = np.where(recall >= target_recall)[0]\n",
        "    \n",
        "    if len(valid_idxs) > 0:\n",
        "        best_idx = valid_idxs[-1] \n",
        "        threshold = thresholds[best_idx]\n",
        "    else:\n",
        "        best_idx = np.argmax(recall)\n",
        "        threshold = thresholds[best_idx]\n",
        "\n",
        "    predictions = (anomaly_scores > threshold).astype(int)\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Threshold escolhido: {threshold:.6f} (Para Recall ~{target_recall:.0%})\")\n",
        "    print(\"\\n--- RELATÃ“RIO FINAL (TESTE) ---\")\n",
        "    print(classification_report(y_test, predictions, target_names=['Normal', 'Fraude']))\n",
        "    \n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    print(f\"Matriz de ConfusÃ£o:\\n{cm}\")\n",
        "\n",
        "    # ExportaÃ§Ã£o\n",
        "    df_output = pd.DataFrame({\n",
        "        'id': ids_test,\n",
        "        'anomaly_score': anomaly_scores,\n",
        "        'is_anomaly': predictions\n",
        "    })\n",
        "\n",
        "    output_file = os.path.join(OUTPUT_PATH, 'autoencoder_predictions.csv')\n",
        "    df_output.to_csv(output_file, index=False)\n",
        "    print(f\"\\nâœ… Arquivo de prediÃ§Ãµes salvo em: {output_file}\")\n",
        "\n",
        "# Executa a avaliaÃ§Ã£o final se houver modelo\n",
        "if best_model:\n",
        "    generate_final_scores(best_model, DATA_PATH, target_recall=0.8)\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado com sucesso.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
